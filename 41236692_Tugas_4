{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtFMim6UK1JtGkU607AoJu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/retnowulandaridwi89-lgtm/Tugas-Deep-Learning-Lanjut/blob/main/41236692_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZkS-rI-01bB",
        "outputId": "8256e18d-8880-44d0-b18d-2b8f0dc8e4d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.19.0 in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n",
            "TensorFlow version: 2.19.0\n",
            "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.19.0\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\",\n",
        "tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Unduh teks Shakespeare\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "\n",
        "text = requests.get(url).text\n",
        "print(f\"Panjang teks: {len(text)} karakter\")\n",
        "print(\"Contoh:\\n\", text[:500])\n",
        "\n",
        "# Bangun vocabulary\n",
        "vocab = sorted(set(text))\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "# Mapping char â†” index\n",
        "char_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
        "idx_to_char = np.array(vocab)\n",
        "# Encode teks menjadi integer\n",
        "text_as_int = np.array([char_to_idx[c] for c in text])\n",
        "\n",
        "# Parameter\n",
        "seq_length = 100 # Panjang konteks\n",
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "\n",
        "# Buat dataset pasangan (input, target)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZGANm_x1jpL",
        "outputId": "02dac145-a682-4cf4-aa81-69febf0783e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Panjang teks: 1115394 karakter\n",
            "Contoh:\n",
            " First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n",
            "Vocabulary size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = self.positional_encoding(max_len, d_model)\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        # Learned positional embedding (lebih sederhana)\n",
        "        return tf.Variable(tf.random.normal((1, position, d_model)))\n",
        "\n",
        "    def call(self, x):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x) * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.pos_encoding[:, :seq_len, :]\n",
        "        return x\n",
        "\n",
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % num_heads == 0\n",
        "        self.depth = d_model // num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # Skala dot-product attention\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        if mask is None:\n",
        "            seq_len = tf.shape(q)[2]\n",
        "            mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "            mask = mask[tf.newaxis, tf.newaxis, :, :]\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "        return self.dense(output)\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = CausalSelfAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        attn_output = self.mha(x, x, x) # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        return out2\n",
        "\n",
        "class GPT(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, dff,\n",
        "                 max_len=1000, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model, max_len)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in\n",
        "                           range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = self.pos_embedding(x) # (batch, seq_len, d_model)\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.dec_layers[i](x, training=training)\n",
        "        logits = self.final_layer(x) # (batch, seq_len, vocab_size)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ifDmsykO1roj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter\n",
        "vocab_size = len(vocab)\n",
        "d_model = 128\n",
        "num_layers = 4\n",
        "num_heads = 8\n",
        "dff = 512\n",
        "max_len = 256 # Increased max_len to accommodate longer generated sequences (e.g., 6 + 200 = 206)\n",
        "# Inisialisasi model\n",
        "model = GPT(vocab_size, d_model, num_layers, num_heads, dff, max_len)\n",
        "# Optimizer dan loss\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    # Opsional: mask padding jika ada (di dataset ini, tidak ada padding sebenarnya)\n",
        "    # Tapi kita pertahankan untuk umum\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inp, training=True)\n",
        "        loss = loss_function(tar, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss # Kembalikan loss untuk logging\n",
        "\n",
        "# Pelatihan\n",
        "EPOCHS = 50\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for batch, (inp, tar) in enumerate(dataset):\n",
        "        loss = train_step(inp, tar)\n",
        "        epoch_loss += loss\n",
        "        num_batches += 1\n",
        "        if batch % 100 == 0:\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Avg Loss {avg_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBNY6hdh3Xkn",
        "outputId": "a2aac472-0403-4876-e680-7d3a104e353a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Avg Loss 4.3251\n",
            "Epoch 1 Batch 100 Avg Loss 3.1530\n",
            "Epoch 2 Batch 0 Avg Loss 2.4158\n",
            "Epoch 2 Batch 100 Avg Loss 2.3431\n",
            "Epoch 3 Batch 0 Avg Loss 2.1857\n",
            "Epoch 3 Batch 100 Avg Loss 2.1395\n",
            "Epoch 4 Batch 0 Avg Loss 2.0309\n",
            "Epoch 4 Batch 100 Avg Loss 1.9869\n",
            "Epoch 5 Batch 0 Avg Loss 1.8867\n",
            "Epoch 5 Batch 100 Avg Loss 1.8779\n",
            "Epoch 6 Batch 0 Avg Loss 1.8266\n",
            "Epoch 6 Batch 100 Avg Loss 1.7961\n",
            "Epoch 7 Batch 0 Avg Loss 1.7283\n",
            "Epoch 7 Batch 100 Avg Loss 1.7380\n",
            "Epoch 8 Batch 0 Avg Loss 1.6795\n",
            "Epoch 8 Batch 100 Avg Loss 1.6905\n",
            "Epoch 9 Batch 0 Avg Loss 1.6778\n",
            "Epoch 9 Batch 100 Avg Loss 1.6517\n",
            "Epoch 10 Batch 0 Avg Loss 1.6309\n",
            "Epoch 10 Batch 100 Avg Loss 1.6197\n",
            "Epoch 11 Batch 0 Avg Loss 1.5639\n",
            "Epoch 11 Batch 100 Avg Loss 1.5903\n",
            "Epoch 12 Batch 0 Avg Loss 1.5745\n",
            "Epoch 12 Batch 100 Avg Loss 1.5684\n",
            "Epoch 13 Batch 0 Avg Loss 1.5663\n",
            "Epoch 13 Batch 100 Avg Loss 1.5457\n",
            "Epoch 14 Batch 0 Avg Loss 1.5362\n",
            "Epoch 14 Batch 100 Avg Loss 1.5289\n",
            "Epoch 15 Batch 0 Avg Loss 1.5263\n",
            "Epoch 15 Batch 100 Avg Loss 1.5114\n",
            "Epoch 16 Batch 0 Avg Loss 1.4952\n",
            "Epoch 16 Batch 100 Avg Loss 1.4963\n",
            "Epoch 17 Batch 0 Avg Loss 1.4896\n",
            "Epoch 17 Batch 100 Avg Loss 1.4804\n",
            "Epoch 18 Batch 0 Avg Loss 1.4355\n",
            "Epoch 18 Batch 100 Avg Loss 1.4694\n",
            "Epoch 19 Batch 0 Avg Loss 1.4475\n",
            "Epoch 19 Batch 100 Avg Loss 1.4575\n",
            "Epoch 20 Batch 0 Avg Loss 1.4464\n",
            "Epoch 20 Batch 100 Avg Loss 1.4474\n",
            "Epoch 21 Batch 0 Avg Loss 1.4055\n",
            "Epoch 21 Batch 100 Avg Loss 1.4405\n",
            "Epoch 22 Batch 0 Avg Loss 1.3907\n",
            "Epoch 22 Batch 100 Avg Loss 1.4311\n",
            "Epoch 23 Batch 0 Avg Loss 1.4228\n",
            "Epoch 23 Batch 100 Avg Loss 1.4220\n",
            "Epoch 24 Batch 0 Avg Loss 1.4346\n",
            "Epoch 24 Batch 100 Avg Loss 1.4144\n",
            "Epoch 25 Batch 0 Avg Loss 1.3973\n",
            "Epoch 25 Batch 100 Avg Loss 1.4095\n",
            "Epoch 26 Batch 0 Avg Loss 1.3913\n",
            "Epoch 26 Batch 100 Avg Loss 1.4003\n",
            "Epoch 27 Batch 0 Avg Loss 1.4220\n",
            "Epoch 27 Batch 100 Avg Loss 1.3953\n",
            "Epoch 28 Batch 0 Avg Loss 1.3711\n",
            "Epoch 28 Batch 100 Avg Loss 1.3869\n",
            "Epoch 29 Batch 0 Avg Loss 1.3902\n",
            "Epoch 29 Batch 100 Avg Loss 1.3829\n",
            "Epoch 30 Batch 0 Avg Loss 1.3836\n",
            "Epoch 30 Batch 100 Avg Loss 1.3780\n",
            "Epoch 31 Batch 0 Avg Loss 1.3696\n",
            "Epoch 31 Batch 100 Avg Loss 1.3737\n",
            "Epoch 32 Batch 0 Avg Loss 1.3638\n",
            "Epoch 32 Batch 100 Avg Loss 1.3715\n",
            "Epoch 33 Batch 0 Avg Loss 1.3471\n",
            "Epoch 33 Batch 100 Avg Loss 1.3590\n",
            "Epoch 34 Batch 0 Avg Loss 1.3281\n",
            "Epoch 34 Batch 100 Avg Loss 1.3580\n",
            "Epoch 35 Batch 0 Avg Loss 1.3366\n",
            "Epoch 35 Batch 100 Avg Loss 1.3560\n",
            "Epoch 36 Batch 0 Avg Loss 1.3422\n",
            "Epoch 36 Batch 100 Avg Loss 1.3508\n",
            "Epoch 37 Batch 0 Avg Loss 1.3549\n",
            "Epoch 37 Batch 100 Avg Loss 1.3449\n",
            "Epoch 38 Batch 0 Avg Loss 1.3060\n",
            "Epoch 38 Batch 100 Avg Loss 1.3412\n",
            "Epoch 39 Batch 0 Avg Loss 1.3867\n",
            "Epoch 39 Batch 100 Avg Loss 1.3377\n",
            "Epoch 40 Batch 0 Avg Loss 1.3259\n",
            "Epoch 40 Batch 100 Avg Loss 1.3332\n",
            "Epoch 41 Batch 0 Avg Loss 1.3102\n",
            "Epoch 41 Batch 100 Avg Loss 1.3323\n",
            "Epoch 42 Batch 0 Avg Loss 1.3388\n",
            "Epoch 42 Batch 100 Avg Loss 1.3316\n",
            "Epoch 43 Batch 0 Avg Loss 1.2843\n",
            "Epoch 43 Batch 100 Avg Loss 1.3285\n",
            "Epoch 44 Batch 0 Avg Loss 1.2833\n",
            "Epoch 44 Batch 100 Avg Loss 1.3247\n",
            "Epoch 45 Batch 0 Avg Loss 1.3165\n",
            "Epoch 45 Batch 100 Avg Loss 1.3213\n",
            "Epoch 46 Batch 0 Avg Loss 1.3094\n",
            "Epoch 46 Batch 100 Avg Loss 1.3169\n",
            "Epoch 47 Batch 0 Avg Loss 1.3271\n",
            "Epoch 47 Batch 100 Avg Loss 1.3167\n",
            "Epoch 48 Batch 0 Avg Loss 1.2665\n",
            "Epoch 48 Batch 100 Avg Loss 1.3097\n",
            "Epoch 49 Batch 0 Avg Loss 1.3079\n",
            "Epoch 49 Batch 100 Avg Loss 1.3111\n",
            "Epoch 50 Batch 0 Avg Loss 1.2551\n",
            "Epoch 50 Batch 100 Avg Loss 1.3071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, length=100, temperature=1.0):\n",
        "    # Encode string awal menjadi indeks\n",
        "    input_ids = [char_to_idx.get(s, 0) for s in start_string] # gunakan 0 jika karakter tidak dikenal\n",
        "    input_ids = tf.expand_dims(input_ids, 0) # shape: (1, seq_len)\n",
        "    text_generated = []\n",
        "    # Jalankan inference autoregressive\n",
        "    for i in range(length):\n",
        "        # Prediksi distribusi probabilitas untuk seluruh urutan\n",
        "        predictions = model(input_ids, training=False) # shape: (1, seq_len, vocab_size)\n",
        "        # Ambil prediksi untuk posisi terakhir\n",
        "        last_pred = predictions[:, -1, :] # shape: (1, vocab_size)\n",
        "        # Terapkan temperature scaling (opsional, untuk variasi)\n",
        "        last_pred = last_pred / temperature\n",
        "        # Sampling dari distribusi\n",
        "        predicted_id = tf.random.categorical(last_pred, num_samples=1) # shape: (1, 1)\n",
        "        predicted_id = tf.squeeze(predicted_id, axis=-1).numpy()[0] # scalar int\n",
        "        # Tambahkan ke input berikutnya\n",
        "        input_ids = tf.concat([input_ids, [[predicted_id]]], axis=1)\n",
        "        # Tambahkan ke hasil\n",
        "        text_generated.append(idx_to_char[predicted_id])\n",
        "    return start_string + ''.join(text_generated)"
      ],
      "metadata": {
        "id": "t25GfMcB3l3u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate teks\n",
        "generated = generate_text(model, start_string=\"ROMEO:\",\n",
        "length=200, temperature=0.8)\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hePkMrig3t6U",
        "outputId": "4e5f6aeb-40e1-4c31-a44e-eb0617fe8dd0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "I am play to well: he may believe my cheeks;\n",
            "And I am bloody rather in their tribunes should\n",
            "d noty t derethind t eand the athite blet mblely winde ps core indefuthind\n",
            "Aus thin h areit the awan;\n",
            "Tury\n"
          ]
        }
      ]
    }
  ]
}
